% !TEX root = autopaxos.tex
% !TEX TS-program = pdflatexmk
% For TeXShop on OS X and Herbert Schulz's latexmk engine.

\newcommand{\Tbf}{T_{\mathit{bf}}}
\newcommand{\Thb}{T_{\mathit{hb}}}
\newcommand{\Tto}{T_{\mathit{to}}}
\newcommand{\Tl}{T_{\mathit{l}}}
\newcommand{\Cr}{C_{\mathit{r}}}
\newcommand{\Chb}{C_{\mathit{hb}}}
\newcommand{\Pff}{P_{\mathit{ff}}}
\newcommand{\Ptf}{P_{\mathit{tf}}}

\section{Design}
We designed our model system around the same abstractions provided by Lamport in his original Paxos paper.
Our system contains modules for Paxos proposers and accepters, as well as client-level server objects.
While notionally separate entities, practically speaking the system can be viewed as a near-monolithic collection of coroutines written using the Tamer \cite{Krohn07} event programming DSL.

We have not streamlined or optimized our implementation of Paxos.
As previously mentioned, our goal is not to demonstrate the fastest or most efficient version of a distributed consensus;
instead, we show that our work can improve the performance of an arbitrary algorithm without any special consideration at the protocol level.

In general, our approach can be broken down into three categories:
1) measurement: we instrument our Paxos code to capture information about its surrounding environment;
2) policy: we separate the concerns of making good decisions about how and what to do with the more mundane concerns of actually accomplishing them;
and 3) tuning: ultimately, any feedback loop needs an actuator for activity, and ours is no different.

\subsection{Measurement}
In order to capture the requisite statistics, we augmented our Paxos implementation with a centralized clearinghouse module we call the \texttt{Telemetry} class.
We are primarily interested in collected two salient pieces of information:
an estimate of the communications latency in the system, and the approximate failure rate of our nodes.

Our Paxos code uses a long-lived master, and heartbeat messages allow subordinates in the system to determine when the master has failed.
Since these messages are regularly sent to all nodes in the system, they are an ideal mechanism for measuring the point-to-point round trip time (RTT).
Similarly, node faults are detected through timeouts on expected messages, usually heartbeat messages and their responses.
We simply mark every timer expiration with code that records relevant information about the failure event to the \texttt{Telementry} class.

It is important to recognize that both of these features are \emph{estimates} of actual behavior.
The system has no oracle to guide it, and we study the accuracy of these methods in section~\ref{evaluation}.

\subsection{Policy}
% policy = minimize cost
% explain cost models (uptime, traffic, recovery)
With more information, we enable informed decision-making.
Our policy module is responsible for converting the raw data from software instrumentation into actionable decisions.
Specifically, we use an analytical cost-model to optimize for performance.
At a high level, the model is just a mathematical way of describing ``goodness.''
While many others would have worked, we chose to express efficacy as the ratio of client availability (simplified as uptime) to cost (data transferred across the network).
To be concrete, we use the set of equations below to quantify what we mean by these terms:

\begin{equation}
	\mathrm{PMetric} = \frac%
{	\Tbf - \Ptf - \Pff } % Uptime
{	\frac{\Chb}{\Thb} + \frac{\Cr}{\Tbf} + \Pff\Cr } % Traffic
\end{equation}

where $\Tbf$ is the mean time between failures (MTBF), $\Tto$ is the master timeout constant, $\Thb$ is the heartbeat interval, $\Chb$ is the cost of sending a single pointwise heartbeat, $\Pff$ and $\Ptf$ are the probabilities of our timeout flagging a false failure or detecting a true failure, respectively, and $\Cr$ is the cost of recovery, that is, the cost of recovering from a master failure.
The subexpressions are $\Pff$ and $\Ptf$ are:

\begin{equation}
	\Pff = \mathrm{CDF}_{N(0,\sigma)} \Thb + \Tl - \Tto
\end{equation}

\begin{equation}
	\Ptf = 1-\mathrm{CDF}_{Exp(\frac{1}{\Tbf-\Tl})} 1000
\end{equation}

where $\Tl$ is the latency and the notation $\mathrm{CDF}_{N(\mu,\sigma)}$ and $\mathrm{CDF}_{\mathrm{Exp}(\lambda)}$ represent the cumulative distribution functions for a normal and exponential distributions.

\subsection{Tuning}
% adjust HB freq
Using this measure, each node in the quorum measures its environment adjusts its own parameters on regular intervals.
While we could have used more advanced techniques, our implementation uses brute force to identify the most viable parameters for a given system.
We simply iterate over the space of all possible parameter combinations until we arrive at a minimum.
We then directly assign values to various locations in memory.
No human intervention is necessary for this, and the process occurs many times per second.
